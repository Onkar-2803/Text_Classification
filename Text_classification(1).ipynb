{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text classification(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1mPbN07Aw68T73Cfh2zrIWkgF-Mj54cS1",
      "authorship_tag": "ABX9TyPHm8NeVFt0fCvYKE+KaUMj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH7ZOFAhS-bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing Libraries\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc7nY1_VudY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Accessing document uploaded \n",
        "\n",
        "path_df = \"/content/drive/My Drive/Pickles/News_dataset.pickle\"\n",
        "\n",
        "with open(path_df, 'rb') as data:\n",
        "    df = pickle.load(data)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Yd6J3ews5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "7dd277a1-866b-4f50-b1fb-66bdcd5cbb29"
      },
      "source": [
        "#checking data\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>Content</th>\n",
              "      <th>Category</th>\n",
              "      <th>Complete_Filename</th>\n",
              "      <th>id</th>\n",
              "      <th>News_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>001.txt</td>\n",
              "      <td>Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...</td>\n",
              "      <td>business</td>\n",
              "      <td>001.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>2569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>002.txt</td>\n",
              "      <td>Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...</td>\n",
              "      <td>business</td>\n",
              "      <td>002.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>2257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>003.txt</td>\n",
              "      <td>Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...</td>\n",
              "      <td>business</td>\n",
              "      <td>003.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>1557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>004.txt</td>\n",
              "      <td>High fuel prices hit BA's profits\\r\\n\\r\\nBriti...</td>\n",
              "      <td>business</td>\n",
              "      <td>004.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>2421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>005.txt</td>\n",
              "      <td>Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...</td>\n",
              "      <td>business</td>\n",
              "      <td>005.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>1575</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  File_Name                                            Content  ... id News_length\n",
              "0   001.txt  Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...  ...  1        2569\n",
              "1   002.txt  Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...  ...  1        2257\n",
              "2   003.txt  Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...  ...  1        1557\n",
              "3   004.txt  High fuel prices hit BA's profits\\r\\n\\r\\nBriti...  ...  1        2421\n",
              "4   005.txt  Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...  ...  1        1575\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPVrmIpGtF1D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "77c0559e-64d3-4d1d-83cb-5272962e537f"
      },
      "source": [
        "#Chcking article\n",
        "\n",
        "df.loc[1]['Content']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Dollar gains on Greenspan speech\\r\\n\\r\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\r\\n\\r\\nAnd Alan Greenspan highlighted the US government\\'s willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan\\'s speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman\\'s taking a much more sanguine view on the current account deficit than he\\'s taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He\\'s taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\\r\\n\\r\\nWorries about the deficit concerns about China do, however, remain. China\\'s currency remains pegged to the dollar and the US currency\\'s sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing\\'s policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve\\'s decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US\\'s yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plFDixlbAk9A",
        "colab_type": "text"
      },
      "source": [
        "1)  Text cleaning and preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH-WzIh0xiT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Text cleaning\n",
        "\n",
        "df['Content_Parsed_1'] = df['Content'].str.replace(\"\\r\", \" \")\n",
        "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
        "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
        "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6M9D2mRzdZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Text preparation\n",
        "\n",
        "df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()         #all to lower case\n",
        "\n",
        "punctuation_signs = list(\"?:!.,;\")                                  #remove punctuations\n",
        "df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
        "\n",
        "for punct_sign in punctuation_signs:\n",
        "    df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')\n",
        "\n",
        "df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")       #remove possessive pronouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCYxU3bGOBab",
        "colab_type": "text"
      },
      "source": [
        "Use any 1 method for Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMB0BmTl11B9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a2e36386-b779-43a5-e608-a11782dc04bf"
      },
      "source": [
        "#Stemming and Lemmatization\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0WaMcetLvke",
        "colab_type": "text"
      },
      "source": [
        "1st method for lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aCUb1qz36GO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stemming and Lemmatization\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "nrows = len(df)\n",
        "lemmatized_text_list = []\n",
        "\n",
        "for row in range(0, nrows):\n",
        "    \n",
        "    # Create an empty list containing lemmatized words\n",
        "    lemmatized_list = []\n",
        "    \n",
        "    # Save the text and its words into an object\n",
        "    text = df.loc[row]['Content_Parsed_4']\n",
        "    text_words = text.split(\" \")\n",
        "\n",
        "    # Iterate through every word to lemmatize\n",
        "    for word in text_words:\n",
        "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
        "        \n",
        "    # Join the list\n",
        "    lemmatized_text = \" \".join(lemmatized_list)\n",
        "    \n",
        "    # Append to the list containing the texts\n",
        "    lemmatized_text_list.append(lemmatized_text)\n",
        "\n",
        "df['Content_Parsed_5'] = lemmatized_text_list    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aCAu8N2FjWl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8fc08618-6436-4f46-eddb-5a205941dcb9"
      },
      "source": [
        "df['Content_Parsed_5']\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       ad sales boost time warner profit quarterly pr...\n",
              "1       dollar gain on greenspan speech the dollar hav...\n",
              "2       yukos unit buyer face loan claim the owners of...\n",
              "3       high fuel price hit ba profit british airways ...\n",
              "4       pernod takeover talk lift domecq share in uk d...\n",
              "                              ...                        \n",
              "2220    bt program to beat dialler scam bt be introduc...\n",
              "2221    spam e-mail tempt net shoppers computer users ...\n",
              "2222    be careful how you code a new european directi...\n",
              "2223    us cyber security chief resign the man make su...\n",
              "2224    lose yourself in online game online role play ...\n",
              "Name: Content_Parsed_5, Length: 2225, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8_fCLACL8uk",
        "colab_type": "text"
      },
      "source": [
        "2nd method for lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCLczhk6FzrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# function to convert nltk tag to wordnet tag\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    #tokenize the sentence and find the POS tag for each token\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
        "    #tuple of (token, wordnet_tag)\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:        \n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "    return \" \".join(lemmatized_sentence)\n",
        "\n",
        "nrows = len(df)\n",
        "lemmatized_text_list = []\n",
        "\n",
        "for row in range(0, nrows):\n",
        "    lemmatized_text = lemmatize_sentence(df.loc[row]['Content_Parsed_4'])\n",
        "    lemmatized_text_list.append(lemmatized_text)\n",
        "\n",
        "df['Content_Parsed_5'] = lemmatized_text_list    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc5sd5UXMMnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Content_Parsed_5']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA9rYSi6rceS",
        "colab_type": "text"
      },
      "source": [
        "Use any 1 method for stop word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFzGE0OldTMu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4d4dec4a-64e0-4505-a3a6-9e6738e8675b"
      },
      "source": [
        "#Downloading\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bumHuGZYeQX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing stop words\n",
        "\n",
        "stop_words = list(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlMwHosAri_I",
        "colab_type": "text"
      },
      "source": [
        "1st Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EevBUtPPn1hR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
        "\n",
        "for stop_word in stop_words:\n",
        "\n",
        "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
        "    df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHNkwlBDrlJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "0b6b4331-ddca-4427-9698-75146f1cdad0"
      },
      "source": [
        "df.loc[5]['Content_Parsed_6']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'japan narrowly escape recession japan economy teeter   brink   technical recession   three months  september figure show revise figure indicate growth   01% -   similar-sized contraction   previous quarter   annual basis  data suggest annual growth   02% suggest  much  hesitant recovery   previously  think  common technical definition   recession  two successive quarter  negative growth  government  keen  play   worry implications   data  maintain  view  japan economy remain   minor adjustment phase   upward climb    monitor developments carefully say economy minister heizo takenaka    face   strengthen yen make export less competitive  indications  weaken economic condition ahead observers  less sanguine  paint  picture   recovery much patchier  previously think say paul sheard economist  lehman brothers  tokyo improvements   job market apparently  yet  fee   domestic demand  private consumption   02%   third quarter'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge00Y4vStKKb",
        "colab_type": "text"
      },
      "source": [
        "2nd Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzbC4XFWrwmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_list_final=[]\n",
        "nrows = len(df)\n",
        "stopwords_english = stopwords.words('english') \n",
        "\n",
        "for row in range(0, nrows):\n",
        "    \n",
        "    # Create an empty list containing no stop words\n",
        "    stop_list = []\n",
        "    \n",
        "    # Save the text and its words into an object\n",
        "    text = df.loc[row]['Content_Parsed_5']\n",
        "    text_words = text.split(\" \")\n",
        "\n",
        "    # Iterate through every word to remove stopwords\n",
        "    for word in text_words:\n",
        "        if (word not in stopwords_english): \n",
        "          stop_list.append(word)\n",
        "        \n",
        "    # Join the list\n",
        "    stop_text = \" \".join(stop_list)\n",
        "    \n",
        "    # Append to the list containing the texts\n",
        "    stop_list_final.append(stop_text)\n",
        "\n",
        "df['Content_Parsed_6'] = stop_list_final   \n",
        "       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBftZQFNiL-J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "f5633a4a-a9a5-4643-90a1-57c61f9eb02a"
      },
      "source": [
        "df.loc[5]['Content_Parsed_6']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'japan narrowly escape recession japan economy teeter brink technical recession three months september figure show revise figure indicate growth 01% - similar-sized contraction previous quarter annual basis data suggest annual growth 02% suggest much hesitant recovery previously think common technical definition recession two successive quarter negative growth government keen play worry implications data maintain view japan economy remain minor adjustment phase upward climb monitor developments carefully say economy minister heizo takenaka face strengthen yen make export less competitive indications weaken economic condition ahead observers less sanguine paint picture recovery much patchier previously think say paul sheard economist lehman brothers tokyo improvements job market apparently yet fee domestic demand private consumption 02% third quarter'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSKIlkjv_Ggi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "294240b4-53ad-46f6-c630-121093055aca"
      },
      "source": [
        "#Checking data\n",
        "\n",
        "df.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>Content</th>\n",
              "      <th>Category</th>\n",
              "      <th>Complete_Filename</th>\n",
              "      <th>id</th>\n",
              "      <th>News_length</th>\n",
              "      <th>Content_Parsed_1</th>\n",
              "      <th>Content_Parsed_2</th>\n",
              "      <th>Content_Parsed_3</th>\n",
              "      <th>Content_Parsed_4</th>\n",
              "      <th>Content_Parsed_5</th>\n",
              "      <th>Content_Parsed_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>001.txt</td>\n",
              "      <td>Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...</td>\n",
              "      <td>business</td>\n",
              "      <td>001.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>2569</td>\n",
              "      <td>Ad sales boost Time Warner profit Quarterly pr...</td>\n",
              "      <td>ad sales boost time warner profit quarterly pr...</td>\n",
              "      <td>ad sales boost time warner profit quarterly pr...</td>\n",
              "      <td>ad sales boost time warner profit quarterly pr...</td>\n",
              "      <td>ad sales boost time warner profit quarterly pr...</td>\n",
              "      <td>ad sales boost time warner profit quarterly pr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  File_Name  ...                                   Content_Parsed_6\n",
              "0   001.txt  ...  ad sales boost time warner profit quarterly pr...\n",
              "\n",
              "[1 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z_AUcVC_-7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing the old content_parsed columns\n",
        "\n",
        "list_columns = [\"File_Name\", \"Category\", \"Complete_Filename\", \"Content\", \"Content_Parsed_6\"]\n",
        "df = df[list_columns]\n",
        "\n",
        "df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T617bG_sARfk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9e139403-bedb-44f7-8b2c-71d3f578613e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>Category</th>\n",
              "      <th>Complete_Filename</th>\n",
              "      <th>Content</th>\n",
              "      <th>Content_Parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>001.txt</td>\n",
              "      <td>business</td>\n",
              "      <td>001.txt-business</td>\n",
              "      <td>Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...</td>\n",
              "      <td>ad sales boost time warner profit quarterly pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>002.txt</td>\n",
              "      <td>business</td>\n",
              "      <td>002.txt-business</td>\n",
              "      <td>Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...</td>\n",
              "      <td>dollar gain  greenspan speech  dollar  hit  hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>003.txt</td>\n",
              "      <td>business</td>\n",
              "      <td>003.txt-business</td>\n",
              "      <td>Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...</td>\n",
              "      <td>yukos unit buyer face loan claim  owners  emba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>004.txt</td>\n",
              "      <td>business</td>\n",
              "      <td>004.txt-business</td>\n",
              "      <td>High fuel prices hit BA's profits\\r\\n\\r\\nBriti...</td>\n",
              "      <td>high fuel price hit ba profit british airways ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>005.txt</td>\n",
              "      <td>business</td>\n",
              "      <td>005.txt-business</td>\n",
              "      <td>Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...</td>\n",
              "      <td>pernod takeover talk lift domecq share  uk dri...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  File_Name  ...                                     Content_Parsed\n",
              "0   001.txt  ...  ad sales boost time warner profit quarterly pr...\n",
              "1   002.txt  ...  dollar gain  greenspan speech  dollar  hit  hi...\n",
              "2   003.txt  ...  yukos unit buyer face loan claim  owners  emba...\n",
              "3   004.txt  ...  high fuel price hit ba profit british airways ...\n",
              "4   005.txt  ...  pernod takeover talk lift domecq share  uk dri...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qReJ8BdgArQK",
        "colab_type": "text"
      },
      "source": [
        "2) Label coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQkPBjSpATxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generating new column for Category codes\n",
        "\n",
        "category_codes = {\n",
        "    'business': 0,\n",
        "    'entertainment': 1,\n",
        "    'politics': 2,\n",
        "    'sport': 3,\n",
        "    'tech': 4\n",
        "}\n",
        "\n",
        "# Category mapping\n",
        "df['Category_Code'] = df['Category']\n",
        "df = df.replace({'Category_Code':category_codes})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAABDQ5XCEkO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1761d274-2ede-4512-e3f0-2aafc3046c21"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>Category</th>\n",
              "      <th>Complete_Filename</th>\n",
              "      <th>Content</th>\n",
              "      <th>Content_Parsed</th>\n",
              "      <th>Category_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>001.txt</td>\n",
              "      <td>business</td>\n",
              "      <td>001.txt-business</td>\n",
              "      <td>Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...</td>\n",
              "      <td>ad sales boost time warner profit quarterly pr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>002.txt</td>\n",
              "      <td>business</td>\n",
              "      <td>002.txt-business</td>\n",
              "      <td>Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...</td>\n",
              "      <td>dollar gain  greenspan speech  dollar  hit  hi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>003.txt</td>\n",
              "      <td>business</td>\n",
              "      <td>003.txt-business</td>\n",
              "      <td>Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...</td>\n",
              "      <td>yukos unit buyer face loan claim  owners  emba...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>004.txt</td>\n",
              "      <td>business</td>\n",
              "      <td>004.txt-business</td>\n",
              "      <td>High fuel prices hit BA's profits\\r\\n\\r\\nBriti...</td>\n",
              "      <td>high fuel price hit ba profit british airways ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>005.txt</td>\n",
              "      <td>business</td>\n",
              "      <td>005.txt-business</td>\n",
              "      <td>Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...</td>\n",
              "      <td>pernod takeover talk lift domecq share  uk dri...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  File_Name  ... Category_Code\n",
              "0   001.txt  ...             0\n",
              "1   002.txt  ...             0\n",
              "2   003.txt  ...             0\n",
              "3   004.txt  ...             0\n",
              "4   005.txt  ...             0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-HGGfsBCtHl",
        "colab_type": "text"
      },
      "source": [
        "3) Train - test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2txuKkYCHh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['Content_Parsed'], \n",
        "                                                    df['Category_Code'], \n",
        "                                                    test_size=0.15, \n",
        "                                                    random_state=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgoVG5EdC0Mk",
        "colab_type": "text"
      },
      "source": [
        "4) Text representation\n",
        "\n",
        "TF-IDF Vectors\n",
        "\n",
        "unigrams & bigrams corresponding to a particular category\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epbPtDKRCw9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameter election\n",
        "ngram_range = (1,2)\n",
        "min_df = 10\n",
        "max_df = 1.\n",
        "max_features = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsXYdtChD21W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d0aedfdf-6e39-44c1-f5fb-881dad4e2122"
      },
      "source": [
        "tfidf = TfidfVectorizer(encoding='utf-8',\n",
        "                        ngram_range=ngram_range,\n",
        "                        stop_words=None,\n",
        "                        lowercase=False,\n",
        "                        max_df=max_df,\n",
        "                        min_df=min_df,\n",
        "                        max_features=max_features,\n",
        "                        norm='l2',\n",
        "                        sublinear_tf=True)\n",
        "                        \n",
        "features_train = tfidf.fit_transform(X_train).toarray()\n",
        "labels_train = y_train\n",
        "print(features_train.shape)\n",
        "\n",
        "features_test = tfidf.transform(X_test).toarray()\n",
        "labels_test = y_test\n",
        "print(features_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1891, 300)\n",
            "(334, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6vdy_FDD6VK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "967d5e00-dfdf-4e9b-fc4c-c9ba964e8bd1"
      },
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np\n",
        "\n",
        "for Product, category_id in sorted(category_codes.items()):\n",
        "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
        "    indices = np.argsort(features_chi2[0])\n",
        "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
        "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "    print(\"# '{}' category:\".format(Product))\n",
        "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
        "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# 'business' category:\n",
            "  . Most correlated unigrams:\n",
            ". market\n",
            ". price\n",
            ". economy\n",
            ". growth\n",
            ". bank\n",
            "  . Most correlated bigrams:\n",
            ". last year\n",
            ". year old\n",
            "\n",
            "# 'entertainment' category:\n",
            "  . Most correlated unigrams:\n",
            ". tv\n",
            ". music\n",
            ". star\n",
            ". award\n",
            ". film\n",
            "  . Most correlated bigrams:\n",
            ". mr blair\n",
            ". prime minister\n",
            "\n",
            "# 'politics' category:\n",
            "  . Most correlated unigrams:\n",
            ". minister\n",
            ". blair\n",
            ". party\n",
            ". election\n",
            ". labour\n",
            "  . Most correlated bigrams:\n",
            ". prime minister\n",
            ". mr blair\n",
            "\n",
            "# 'sport' category:\n",
            "  . Most correlated unigrams:\n",
            ". win\n",
            ". side\n",
            ". game\n",
            ". team\n",
            ". match\n",
            "  . Most correlated bigrams:\n",
            ". say mr\n",
            ". year old\n",
            "\n",
            "# 'tech' category:\n",
            "  . Most correlated unigrams:\n",
            ". digital\n",
            ". technology\n",
            ". computer\n",
            ". software\n",
            ". users\n",
            "  . Most correlated bigrams:\n",
            ". year old\n",
            ". say mr\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7JOkhhhEnV_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "929737ad-bfcd-4c4a-f74a-454512c5f503"
      },
      "source": [
        "bigrams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tell bbc', 'last year', 'prime minister', 'mr blair', 'year old', 'say mr']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQAtmDipneir",
        "colab_type": "text"
      },
      "source": [
        "Unigrams are more relevnat to the category as compared with bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpRcdYL8IhlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train\n",
        "with open('/content/drive/My Drive/Pickles/X_train.pickle', 'wb') as output:\n",
        "    pickle.dump(X_train, output)\n",
        "    \n",
        "# X_test    \n",
        "with open('/content/drive/My Drive/Pickles/X_test.pickle', 'wb') as output:\n",
        "    pickle.dump(X_test, output)\n",
        "    \n",
        "# y_train\n",
        "with open('/content/drive/My Drive/Pickles/y_train.pickle', 'wb') as output:\n",
        "    pickle.dump(y_train, output)\n",
        "    \n",
        "# y_test\n",
        "with open('/content/drive/My Drive/Pickles/y_test.pickle', 'wb') as output:\n",
        "    pickle.dump(y_test, output)\n",
        "    \n",
        "# df\n",
        "with open('/content/drive/My Drive/Pickles/df.pickle', 'wb') as output:\n",
        "    pickle.dump(df, output)\n",
        "    \n",
        "# features_train\n",
        "with open('/content/drive/My Drive/Pickles/features_train.pickle', 'wb') as output:\n",
        "    pickle.dump(features_train, output)\n",
        "\n",
        "# labels_train\n",
        "with open('/content/drive/My Drive/Pickles/labels_train.pickle', 'wb') as output:\n",
        "    pickle.dump(labels_train, output)\n",
        "\n",
        "# features_test\n",
        "with open('/content/drive/My Drive/Pickles/features_test.pickle', 'wb') as output:\n",
        "    pickle.dump(features_test, output)\n",
        "\n",
        "# labels_test\n",
        "with open('/content/drive/My Drive/Pickles/labels_test.pickle', 'wb') as output:\n",
        "    pickle.dump(labels_test, output)\n",
        "    \n",
        "# TF-IDF object\n",
        "with open('/content/drive/My Drive/Pickles/tfidf.pickle', 'wb') as output:\n",
        "    pickle.dump(tfidf, output)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}